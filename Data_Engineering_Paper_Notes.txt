- Medium:
	
	-  "Data pipelines, Luigi, Airflow: everything you need to know"

		Link: https://towardsdatascience.com/data-pipelines-luigi-airflow-everything-you-need-to-know-18dc741449b7
		Description: Detailed comparision of two Workflow Management Systems (WMS) Apache Airflow and Luigi.
		Technologies: Apache Airflow, Luigi 
		More: In Airflow, a workflow is defined as a collection of tasks with directional dependencies, basically a directed acyclic graph (DAG). Each node in the graph is a task, and edges define dependencies among the tasks. The main components of Airflow are: 1) Metadata Database (i.e Postgres), 2) Scheduler, 3) Executor. The metadata database stores the state of tasks and workflows. The scheduler uses the DAGs definitions, together with the state of tasks in the metadata database, and decides what needs to be executed.
		The executor is a message queuing process (usually Celery) which decides which worker will execute each task. No information is shared between tasks, the maximum possbile parallelisation. Tasks do not communicate; has its own scheduler; very nice UI.
		Luigi is a python package to build complex pipelines and it was developed at Spotify and it consist of two main parts: Tasks and Targets. It’s generally based on pipelines, tasks input and output share information and is connected together and exploits Target-based approach. No user interaction with running processes; absent of triggering.


- Trivago:

	- "Accomodation Consilidation: How we created an ETL pipeline on cloud" 
	
		Link: https://tech.trivago.com/2020/03/26/accommodation-consolidation-how-we-created-an-etl-pipeline-on-cloud/	
		Description: How to process millions of updates simulataneously. 
		Technologies:  AWS Glue (ELT tool) and AWS Step Functions (orchestration service)
		More: Initially, we had two proposals for the technologies we could use. We picked AWS Glue because it can process a huge amount of data. AWS Glue is a fully managed ETL service provided by AWS that uses Apache Spark clusters underneath, which seemed perfect to process the large number of updates. We also picked AWS Step Functions because it provides more flexibility in plugging new models. AWS Step Functions is a tool to orchestrate different AWS services, which is ideal to accomplish the flexibility we require. At the end of the prototyping phase, we realized that these two technologies have both benefits and drawbacks for our use-case, hence a hybrid solution using both AWS Glue and AWS Step Functions.
		
	- "Getting Ready For The Big Data Apocalypse" 
	
		Link: https://tech.trivago.com/2019/12/16/getting-ready-for-the-big-data-apocalypse/
		Description: Exponential data growth and how to deal with it. 
		Technologies: Apache Kudo (storage), Apache Impala (SQL engine), Kotlin, gRPC (microservices management).
		More: Data storage - Apache Kudo (open source columnar storage system developed for the Apache Hadoop) that supports BI on Hadoop (but doesn’t provide an SQL engine). Therefore we decided to implement Impala as an SQL engine, which matches perfectly with Kudu and the whole Hadoop ecosystem. The project architecture is based on microservices architecture, which, put simply, splits the project into small independent pieces of code that share the same business logic.
		


		