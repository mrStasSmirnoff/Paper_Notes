## Medium:

#### Listing Embeddings in Search Ranking (AirBnb)

- Link: https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e
- Description: The idea is to train vector representations (Embeddings) of the available accomodations (listings) in Airbnb that will allow to measure similarities between them and eventually to present similar to the users (Similar Listings) or use as the extra features to improve Search Ranking model.
- Technologies: NN (to train with backprop embeddings)
- More:  Given this data set, the aim is to learn a 32-dimensional representation V(L_(i)) of each unique listing Li, such that similar listings lie nearby in the embedding space. The *Negative Sampling* techinique is used to train embeddings. Starting with initially random vectors and proceeds to update them via SGD by reading through search sessions in a sliding window manner. At each step the vector of the central listing is updated by pushing it closer to vectors of positive context listings: listings that were clicked by the same user before and after central listing, within a window of length m (m = 5), and pushing it away from negative context listings: randomly sampled listings (as chances are these are not related to the central listing).  Two modification were also made for this approach:
    - using *Booked Listing as Global Context*: The sessions that end with the user booking the listing are used to adapt the optimization such that at each step we predict not only the neighboring clicked listings but also the eventually booked listing as well.
    - adapting to *Congregated Search*: Users of online travel booking sites typically search only within a single market (same geolocation). As a consequence, for a given central listing, the positive context listings mostly consist of listings from the same market, while the negative context listings mostly consist of listings that are not from the same market as they are sampled randomly from the entire listing vocabulary. This imbalance leads to learning sub-optimal within-market similarities. To address this issue it is proposed to add a set of random negatives Dmn, sampled from the market of the central.
    
    For new accommodations *Cold Start Embeddings* authors find 3 geographically closest listings that do have embeddings, and are of the same listing type and price range as the new listing, and calculate their mean vector. The offline evaluation is done by taking the most recently clicked listing and the listing candidates that need to be ranked, which contain the listing that the user eventually booked. By calculating cosine similarities between embeddings of the clicked listing and the candidate listings we can rank the candidates and observe the rank position of the booked listing. For the *Search Ranking* where the aim is to show to the guest more listings that are similar to the ones we think they liked since starting the search session and fewer listings similar to the ones we think they did not like. To achieve this, for each user authors collected and maintain in real-time (using Kafka) two sets of short-term history events: **Hc** - set of listing ids that user clicked in last 2 weeks and **Hs** - set of listing ids that user skipped in last 2 weeks, where we define skipped listings as ones that were ranked high but skipped by a user in favor of a click on a lower positioned listing. These two similarity measures were next introduced as an additional signal that our Search Ranking Machine Learning model considers when ranking candidate listings.


#### How does Spotify know you so well?

- Link: https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe

- Description: A very broad description of the ML components of Spotify recommendations (Discover weekly)
- Technologies: Colaborative Filtering, NLP (on text), CNN (audio)
- More: In a nutshell, the author depicts three main ML components of Spotify recommendations, namely: Collaborative filtering, NLP on text, and NLP on the audio itself. For the Collaborative filtering the idea is pretty straight-forward, one big 140x30 (mln) matrix of users-songs data is factorized into two matrices *X* (user vector) and *Y* (songs vector). Then the similarities are computed to get "the match". For the second one, it is a bit more interesting. The source for this model is track metadata, news articles, blogs, and other text around the internet. Spotify crawls the web constantly looking for blog posts and other written text about music to figure out what people are saying about specific artists and songs — which adjectives and what particular language is frequently used in reference to those artists and songs, and which other artists and songs are also being discussed alongside them. What happens then is that (most probably) data up into what they call “cultural vectors” or “top terms.” Each artist and song had thousands of top terms that changed on a daily. Each term had an associated weight, which correlated to its relative importance — roughly, the probability that someone will describe the music or artist with that term. Then these terms and weights are used to create a vector representation of the song that can be used to determine if two pieces of music are similar. And the third component - Raw audio data. It allows overcoming the main weakness of Collaborative filtering - lack of generalization (raw audio models take new songs into account). CNN is used with the spectrogram as input. After processing, the neural network spits out an understanding of the song, including signals like estimated time signature, key, mode, tempo, and loudness. Which are also used as features to compute similarities between songs.


## Trivago:

#### Machine Learning and Bathtubs - How Small Visual Changes Improve User Experience" 
	
- Link: https://tech.trivago.com/2019/08/21/machine-learning-and-bathtubs-how-small-visual-changes-improve-user-experience/	
- Description: The idea is to adjust the main images of hotels to reflect the user intent coming from SEM ads containing spa concepts (concept-based main images)
- Technologies: AWS stack for pipelines, RNN, CNN with transfer learning. 
- More: Automated (with CV) image labeling of Trivago 100+ million active images for Spa & Wellness. For training, some public data has been used along with existing handpicked image tags from our trivago hoteliers. Questionnaires have been prepared based on Amazon Mechanical Turk to align the images semantically. The resulting dataset had 2,000 images for each of the categories of "Sauna", "Pool", "Gym", "Yoga", "Hot-tub" and "Massage". Five different CNN architectures (InceptionV3, VGG16, Xception, ResNet50, InceptionResnetV2) were evaluated with VGG16 chosen as the final model. Precision was used as an evaluation metric (to min False Positives). To deal with the images that had to be negated (out all classes that are beyond this concept) two additional (unbalanced) classes were introduced "Bedroom" and "Non-Spa". An average  Precision and Recall of ~85% and ~89% for the individual Spa & Wellness classes.
